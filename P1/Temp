Prac-1 (AAI) Neural Network
x=float(input("Enter value of x:"))
w=float(input("Enter value of w:"))
b=float(input("Enter value of b:"))
net = int(w*x+b)
if(net<0):
    out=0
elif((net>=0)&(net<=1)):
    out=net
else:
    out=1
print("net=",net)
print("output=",out)
#number of elements as input
n = int(input("Enter number of elements:"))
print("Enter the inputs:")
inputs= [] #creating an empty list for inputs
#iternating till the range
for i in range(0,n):
    ele = float(input())
    inputs.append(ele) #adding the element
print(inputs)
print("Enter the weights:")
weights= [] #creating an empty list for inputs
#iternating till the range
for i in range(0,n):
    ele = float(input())
    weights.append(ele) #adding the element
print(weights)
print("The net input can be calculated as Yin = x1w1 + x2w2 + x3w3")
# In[5]
Yin= []
for i in range(0,n):
    Yin.append(inputs[i]*weights[i])
print(round(sum(Yin),3))

Prac-2 (AAI) McCulloch-Pitt 
import numpy
# enter the no of values
num_ip = int(input("Enter the number of inputs:"))
# set the weights with value 1
w1= 1
w2= 1
print("For the",num_ip,"inputs calculate the net input using yin= x1w1 + x2w2")
x1= []
x2= []
for j in range(0,num_ip):
    e1e1 = int(input("x1 = "))
    e1e2 = int(input("x2 = "))
    x1.append(e1e1)
    x2.append(e1e2)
print("x1 = ",x1)
print("x2 = ",x2)
n = x1 * w1
m = x2 * w2
Yin = []
for i in range(0,num_ip):
    Yin.append(n[i] + m[i])
print("Yin =", Yin)    
#Assume one weight as excitatory and the other as inhibitory, i.e,
Yin = []
for i in range(0,num_ip):
    Yin.append(n[i] - m[i])
print("After assuming one weight as excitatory and the other as inhibitory Yin=", Yin)
#from the calculated net inputs, now it is possible yo fire the neuron for input(1,-1)
# only by fixing a threshold of 1, i.e., 0 > 1 for Y unit
# Thus, w1 = 1, w2= -1, 0>=1
Y=[]
for i in range(0,num_ip):
    if(Yin[i]>=1):
        ele=1
        Y.append(ele)
    if(Yin[i]<1):
        ele=0
        Y.append(ele)
print("Y=",Y)

Prac-3 (AAI) Decision Tree
import pandas as pd
#loading the Playtennis data
PlayTennis = pd.read_csv("D:\MSC.IT\Practical\AAI\Datasets\play_tennis.csv")
PlayTennis
PlayTennis.shape
PlayTennis.describe()
PlayTennis.dtypes
PlayTennis.head(4)
PlayTennis.tail(4)
PlayTennis.columns
from sklearn.preprocessing import LabelEncoder
Le = LabelEncoder()
PlayTennis['day'] = Le.fit_transform(PlayTennis ['day'])
PlayTennis['outlook'] = Le.fit_transform(PlayTennis ['outlook'])
PlayTennis['temp'] = Le.fit_transform (PlayTennis ['temp'])
PlayTennis['humidity'] = Le.fit_transform(PlayTennis['humidity'])
PlayTennis['wind'] = Le.fit_transform (PlayTennis ['wind'])
PlayTennis['play'] = Le.fit_transform (PlayTennis['play'])
PlayTennis
y = PlayTennis['play']
x = PlayTennis.drop(['play'], axis=1)
x.dtypes
from sklearn import tree
clf = tree.DecisionTreeClassifier(criterion = 'entropy')
clf = clf.fit(x,y)
tree.plot_tree(clf)

Prac-4 (AAI) SVM
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LogisticRegression, LogisticRegressionCV
from sklearn.svm import SVC,LinearSVC
import plotly.express as pl
from sklearn.model_selection import train_test_split,cross_val_score,cross_val_predict
from sklearn.metrics import confusion_matrix
pd.set_option('display.max_rows',1000)
pd.set_option('display.max_columns',1000)
pd.set_option('display.width',1000)
data = pd.read_csv('D:\MSC.IT\Practical\AAI\Datasets\diabetes.csv')
data.head()
data.tail()
data.shape
data.describe()
x=data.drop('Outcome',axis=1)
y = data["Outcome"] #will predict outcome (diabetes)
x_train = x.iloc[:600]
x_test = x.iloc[600:]
y_train = y[:600]
y_test = y[600:]
print("x_train Shape: ", x_train.shape)
print("x_test Shape: ", x_test.shape)
print("y_train Shape: ", y_train.shape)
print("y_test Shape: ", y_test.shape)
support_vector_classifier=SVC(kernel="linear").fit(x_train,y_train)
support_vector_classifier
support_vector_classifier.C
support_vector_classifier
y_pred=support_vector_classifier.predict(x_test)
cm=confusion_matrix(y_test,y_pred)
cm

Prac-5 (AAI) K-Nearest Neighbor
#WAP to implement k-nearest neighbour algorithm to classify the iris dataset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
plt.style.use('ggplot')
df = pd.read_csv('D:\MSC.IT\Practical\AAI\Datasets\diabetes.csv')
df.head()
df.shape
df.dtypes
x= df.drop('Outcome', axis=1).values
y= df['Outcome'].values
from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.4,random_state=42)
from sklearn.neighbors import KNeighborsClassifier
neighbors = np.arange(1,9)
train_accuracy = np.empty(len(neighbors))
test_accuracy = np.empty(len(neighbors))
for i,k in enumerate(neighbors):
#Setup a knn classifier with k neighbors
    knn = KNeighborsClassifier(n_neighbors=k)
#Fit the model
    knn.fit(x_train, y_train)
#Compute accuracy on the training set
    train_accuracy[i] = knn.score(x_train,y_train)
#Compute accuracy on the test set
    test_accuracy[i] = knn.score(x_test,y_test)
plt.title('k-NN Varying number of neighbors')
plt.plot(neighbors,test_accuracy,label='Testing Accuracy')
plt.plot(neighbors,train_accuracy,label='Training Accuracy')
plt.legend()
plt.xlabel('Number of neighbors')
plt.ylabel('Accuracy')
plt.show()
knn= KNeighborsClassifier(n_neighbors=7)
knn.fit(x_train,y_train)
KNeighborsClassifier(n_neighbors=7)
knn.score(x_test,y_test)
from sklearn.metrics import confusion_matrix
y_pred = knn.predict(x_test)
confusion_matrix(y_test,y_pred)
from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))
y_pred_proba = knn.predict_proba (x_test)[:,1]
from sklearn.metrics import roc_curve
fpr, tpr, thresholds = roc_curve (y_test, y_pred_proba)
plt.plot([0,1],[0,1], 'k--')
plt.plot(fpr, tpr, label='Knn')
plt.xlabel('fpr')
plt.ylabel('tpr')
plt.title('Knn(n_neighbors=7) ROC curve')
plt.show()
from sklearn.metrics import roc_auc_score
roc_auc_score
(y_test,y_pred_proba)
from sklearn.metrics import roc_auc_score
roc_auc_score
(y_test,y_pred_proba)
knn = KNeighborsClassifier()
knn_cv= GridSearchCV (knn, param_grid,cv=5)
knn_cv.fit(x,y)
knn_cv.best_score_
knn_cv.best_params_

Prac-6 (AAI) Naive Bayes
import pandas as pd
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score
from sklearn.naive_bayes import MultinomialNB ,CategoricalNB
from sklearn.metrics import classification_report,accuracy_score,confusion_matrix
#importing dataset
iris= pd.read_csv("D:\MSC.IT\Practical\AAI\Datasets\Iris.csv")
iris
#print top 5
iris.head(5)
#print last 5
iris.tail(5)
iris.shape
iris.describe
iris.info()
iris.columns
iris.dtypes
#selecting all values of 'SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm'
X = iris.iloc[:,[1,2,3,4]].values
#Selecting all values of species (Prediction output)
y = iris.iloc[:,[5]].values
x_train, x_test , y_train, y_test = train_test_split(X,y,test_size=0.2, random_state=40)
print("Split the dataset (X and Y) into training and testing sets successfully")
print("**********FOR GAUSSAIN NAIVE BAYES CLASSIFICATION*************")
GNBclf= GaussianNB()
GNBclf.fit(x_train,y_train)
y_pred = GNBclf.predict(x_test)
out = GNBclf.predict([[5.9,3.0,5.1,1.8]])
print("Predicted Species:",out)
print("Confusion Matrix\n",confusion_matrix(y_test,y_pred))
print("classification Matrix\n",classification_report(y_test,y_pred))
print("Accuracy\n",accuracy_score(y_test,y_pred))

Prac-7 (AAI) K-Means
import pandas as pd
#selecting only longitude, latitude, median_house_value from all the columns
home_data = pd.read_csv('D:\MSC.IT\Practical\AAI\Datasets\housing.csv', usecols = ['longitude', 'latitude', 'median_house_value'])
home_data.head()
import seaborn as sns
#shows the actual map with the help of longitude and latitude
#colored datapoints w.r.t long and lat(which shows house near to ocean is pretty expensive)
sns.scatterplot(data = home_data, x = 'longitude', y = 'latitude', hue = 'median_house_value')
from sklearn.model_selection import train_test_split
#Making testing and training data
X_train, X_test, y_train, y_test = train_test_split(home_data[['latitude', 'longitude']], home_data[['median_house_value']], test_size=0.33, random_state=0)
from sklearn import preprocessing
#Normalising the data
X_train_norm = preprocessing.normalize(X_train)
X_test_norm = preprocessing.normalize(X_test)
from sklearn import preprocessing
#Normalising the data
X_train_norm = preprocessing.normalize(X_train)
X_test_norm = preprocessing.normalize(X_test)
sns.scatterplot(data = X_train, x = 'longitude', y = 'latitude', hue = kmeans.labels_)
sns.boxplot(x = kmeans.labels_, y = y_train['median_house_value'])
from sklearn.metrics import silhouette_score
silhouette_score(X_train_norm, kmeans.labels_, metric='euclidean')
K = range(2, 8)
fits = []
score = []
for k in K:
    # train the model for current value of k on training data
    model = KMeans(n_clusters = k, random_state = 0, n_init='auto').fit(X_train_norm)
    
    # append the model to fits
    fits.append(model)
    
    # Append the silhouette score to scores
    score.append(silhouette_score(X_train_norm, model.labels_, metric='euclidean'))
sns.scatterplot(data = X_train, x = 'longitude', y = 'latitude', hue = fits[0].labels_)
sns.scatterplot(data = X_train, x = 'longitude', y = 'latitude', hue = fits[1].labels_)
sns.scatterplot(data = X_train, x = 'longitude', y = 'latitude', hue = fits[2].labels_)
sns.lineplot(x = K, y = score)
#This is the last part
sns.scatterplot(data = X_train, x = 'longitude', y = 'latitude', hue = fits[3].labels_)
sns.boxplot(x = fits[3].labels_, y = y_train['median_house_value'])
